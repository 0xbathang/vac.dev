{"pageProps":{"markdown":{"content":"\nThis post will introduce Waku. Waku is a fork of Whisper that attempts to\naddresses some of Whisper's shortcomings in an iterative fashion. We will also\nintroduce a theoretical scaling model for Whisper that shows why it doesn't\nscale, and what can be done about it.\n\n## Introduction\n\nWhisper is a gossip-based communication protocol or an ephemeral key-value store\ndepending on which way you look at it. Historically speaking, it is the\nmessaging pilllar of [Web3](http://gavwood.com/dappsweb3.html), together with\nEthereum for consensus and Swarm for storage.\n\nWhisper, being a somewhat esoteric protocol and with some fundamental issues,\nhasn't seen a lot of usage. However, applications such as Status are using it,\nand have been making minor ad hoc modifications to it to make it run on mobile\ndevices.\n\nWhat are these fundamental issues? In short:\n\n1. scalability, most immediately when it comes to bandwidth usage\n2. spam-resistance, proof of work is a poor mechanism for heterogeneous nodes\n3. no incentivized infrastructure, leading to centralized choke points\n4. lack of formal and unambiguous specification makes it hard to analyze and implement\n5. running over devp2p, which limits where it can run and how\n\nIn this post, we'll focus on the first problem, which is scalability through bandwidth usage.\n\n## Whisper theoretical scalability model\n\n*(Feel free to skip this section if you want to get right to the results).*\n\nThere's widespread implicit knowledge that Whisper \"doesn't scale\", but it is less understood exactly why. This theoretical model attempts to encode some characteristics of it. Specifically for use case such as one by Status (see [Status Whisper usage\nspec](https://specs.status.im/spec/3)).\n\n### Caveats\n\nFirst, some caveats: this model likely contains bugs, has wrong assumptions, or completely misses certain dimensions. However, it acts as a form of existence proof for unscalability, with clear reasons.\n\nIf certain assumptions are wrong, then we can challenge them and reason about them in isolation. It doesn’t mean things will definitely work as the model predicts, and that there aren’t unknown unknowns.\n\nThe model also only deals with receiving bandwidth for end nodes, uses mostly static assumptions of averages, and doesn’t deal with spam resistance, privacy guarantees, accounting, intermediate node or network wide failures.\n\n### Goals\n\n1. Ensure network scales by being user or usage bound, as opposed to bandwidth growing in proportion to network size.\n2. Staying with in a reasonable bandwidth limit for limited data plans.\n3. Do the above without materially impacting existing nodes.\n\nIt proceeds through various case with clear assumptions behind them, starting from the most naive assumptions. It shows results for 100 users, 10k users and 1m users.\n\n### Model\n\n```\nCase 1. Only receiving messages meant for you [naive case]\n\nAssumptions:\n- A1. Envelope size (static): 1024kb\n- A2. Envelopes / message (static): 10\n- A3. Received messages / day (static): 100\n- A4. Only receiving messages meant for you.\n\nFor 100 users, receiving bandwidth is 1000.0KB/day\nFor 10k users, receiving bandwidth is 1000.0KB/day\nFor  1m users, receiving bandwidth is 1000.0KB/day\n\n------------------------------------------------------------\n\nCase 2. Receiving messages for everyone [naive case]\n\nAssumptions:\n- A1. Envelope size (static): 1024kb\n- A2. Envelopes / message (static): 10\n- A3. Received messages / day (static): 100\n- A5. Received messages for everyone.\n\nFor 100 users, receiving bandwidth is   97.7MB/day\nFor 10k users, receiving bandwidth is    9.5GB/day\nFor  1m users, receiving bandwidth is  953.7GB/day\n\n------------------------------------------------------------\n\nCase 3. All private messages go over one discovery topic [naive case]\n\nAssumptions:\n- A1. Envelope size (static): 1024kb\n- A2. Envelopes / message (static): 10\n- A3. Received messages / day (static): 100\n- A6. Proportion of private messages (static): 0.5\n- A7. Public messages only received by relevant recipients (static).\n- A8. All private messages are received by everyone (same topic) (static).\n\nFor 100 users, receiving bandwidth is   49.3MB/day\nFor 10k users, receiving bandwidth is    4.8GB/day\nFor  1m users, receiving bandwidth is  476.8GB/day\n\n------------------------------------------------------------\n\nCase 4. All private messages are partitioned into shards [naive case]\n\nAssumptions:\n- A1. Envelope size (static): 1024kb\n- A2. Envelopes / message (static): 10\n- A3. Received messages / day (static): 100\n- A6. Proportion of private messages (static): 0.5\n- A7. Public messages only received by relevant recipients (static).\n- A9. Private messages partitioned across partition shards (static), n=5000\n\nFor 100 users, receiving bandwidth is 1000.0KB/day\nFor 10k users, receiving bandwidth is    1.5MB/day\nFor  1m users, receiving bandwidth is   98.1MB/day\n\n------------------------------------------------------------\n\nCase 5. 4 + Bloom filter with false positive rate\n\nAssumptions:\n- A1. Envelope size (static): 1024kb\n- A2. Envelopes / message (static): 10\n- A3. Received messages / day (static): 100\n- A6. Proportion of private messages (static): 0.5\n- A7. Public messages only received by relevant recipients (static).\n- A9. Private messages partitioned across partition shards (static), n=5000\n- A10. Bloom filter size (m) (static): 512\n- A11. Bloom filter hash functions (k) (static): 3\n- A12. Bloom filter elements, i.e. topics, (n) (static): 100\n- A13. Bloom filter assuming optimal k choice (sensitive to m, n).\n- A14. Bloom filter false positive proportion of full traffic, p=0.1\n\nFor 100 users, receiving bandwidth is   10.7MB/day\nFor 10k users, receiving bandwidth is  978.0MB/day\nFor  1m users, receiving bandwidth is   95.5GB/day\n\nNOTE: Traffic extremely sensitive to bloom false positives\nThis completely dominates network traffic at scale.\nWith p=1% we get 10k users ~100MB/day and 1m users ~10gb/day)\n\n------------------------------------------------------------\n\nCase 6. Case 5 + Benign duplicate receives\n\nAssumptions:\n- A1. Envelope size (static): 1024kb\n- A2. Envelopes / message (static): 10\n- A3. Received messages / day (static): 100\n- A6. Proportion of private messages (static): 0.5\n- A7. Public messages only received by relevant recipients (static).\n- A9. Private messages partitioned across partition shards (static), n=5000\n- A10. Bloom filter size (m) (static): 512\n- A11. Bloom filter hash functions (k) (static): 3\n- A12. Bloom filter elements, i.e. topics, (n) (static): 100\n- A13. Bloom filter assuming optimal k choice (sensitive to m, n).\n- A14. Bloom filter false positive proportion of full traffic, p=0.1\n- A15. Benign duplicate receives factor (static): 2\n- A16. No bad envelopes, bad PoW, expired, etc (static).\n\nFor 100 users, receiving bandwidth is   21.5MB/day\nFor 10k users, receiving bandwidth is    1.9GB/day\nFor  1m users, receiving bandwidth is  190.9GB/day\n\n------------------------------------------------------------\n\nCase 7. 6 + Mailserver under good conditions; small bloom fp; mostly offline\n\nAssumptions:\n- A1. Envelope size (static): 1024kb\n- A2. Envelopes / message (static): 10\n- A3. Received messages / day (static): 100\n- A6. Proportion of private messages (static): 0.5\n- A7. Public messages only received by relevant recipients (static).\n- A9. Private messages partitioned across partition shards (static), n=5000\n- A10. Bloom filter size (m) (static): 512\n- A11. Bloom filter hash functions (k) (static): 3\n- A12. Bloom filter elements, i.e. topics, (n) (static): 100\n- A13. Bloom filter assuming optimal k choice (sensitive to m, n).\n- A14. Bloom filter false positive proportion of full traffic, p=0.1\n- A15. Benign duplicate receives factor (static): 2\n- A16. No bad envelopes, bad PoW, expired, etc (static).\n- A17. User is offline p% of the time (static) p=0.9\n- A18. No bad request, dup messages for mailservers; overlap perfect (static).\n- A19. Mailserver requests can change false positive rate to be p=0.01\n\nFor 100 users, receiving bandwidth is    3.9MB/day\nFor 10k users, receiving bandwidth is  284.8MB/day\nFor  1m users, receiving bandwidth is   27.8GB/day\n\n------------------------------------------------------------\n\nCase 8. No metadata protection w bloom filter; 1 node connected; static shard\n\nAka waku mode.\n\nNext step up is to either only use contact code, or shard more aggressively.\nNote that this requires change of other nodes behavior, not just local node.\n\nAssumptions:\n- A1. Envelope size (static): 1024kb\n- A2. Envelopes / message (static): 10\n- A3. Received messages / day (static): 100\n- A6. Proportion of private messages (static): 0.5\n- A7. Public messages only received by relevant recipients (static).\n- A9. Private messages partitioned across partition shards (static), n=5000\n\nFor 100 users, receiving bandwidth is 1000.0KB/day\nFor 10k users, receiving bandwidth is    1.5MB/day\nFor  1m users, receiving bandwidth is   98.1MB/day\n\n------------------------------------------------------------\n```\n\nSee [source](https://github.com/vacp2p/research/tree/master/whisper_scalability)\nfor more detail on the model and its assumptions.\n\n### Takeaways\n\n1. Whisper as it currently works doesn’t scale, and we quickly run into unacceptable bandwidth usage.\n2. There are a few factors of this, but largely it boils down to noisy topics usage and use of bloom filters. Duplicate (e.g. see [Whisper vs PSS](https://our.status.im/whisper-pss-comparison/)) and bad envelopes are also factors, but this depends a bit more on specific deployment configurations.\n3. Waku mode (case 8) is an additional capability that doesn’t require other nodes to change, for nodes that put a premium on performance.\n4. The next bottleneck after this is the partitioned topics (app/network specific), which either needs to gracefully (and potentially quickly) grow, or an alternative way of consuming those messages needs to be deviced.\n\n![](/img/whisper_scalability.png)\n\nThe results are summarized in the graph above. Notice the log-log scale. The\ncolored backgrounds correspond to the following bandwidth usage:\n\n- Blue: <10mb/d (<~300mb/month)\n- Green: <30mb/d (<~1gb/month)\n- Yellow: <100mb/d (<~3gb/month)\n- Red: >100mb/d (>3gb/month)\n\nThese ranges are somewhat arbitrary, but are based on [user\nrequirements](https://github.com/status-im/status-react/issues/9081) for users\non a limited data plan, with comparable usage for other messaging apps.\n\n## Introducing Waku\n\n### Motivation for a new protocol\n\nApps such as Status will likely use something like Whisper for the forseeable\nfuture, and we want to enable them to use it with more users on mobile devices\nwithout bandwidth exploding with minimal changes.\n\nAdditionally, there's not a clear cut alternative that maps cleanly to the\ndesired use cases (p2p, multicast, privacy-preserving, open, etc).\n\nWe are actively researching, developing and collaborating with more greenfield\napproaches. It is likely that Waku will either converge to those, or Waku will\nlay the groundwork (clear specs, common issues/components) necessary to make\nswitching to another protocol easier. In this project we want to emphasize\niterative work with results on the order of weeks.\n\n### Briefly on Waku mode\n\n- Doesn’t impact existing clients, it’s just a separate node and capability.\n- Other nodes can still use Whisper as is, like a full node.\n- Sacrifices metadata protection and incurs higher connectivity/availability requirements for scalbility\n\n**Requirements:**\n\n- Exposes API to get messages from a set of list of topics (no bloom filter)\n- Way of being identified as a Waku node (e.g. through version string)\n- Option to statically encode this node in app, e.g. similar to custom bootnodes/mailserver\n- Only node that needs to be connected to, possibly as Whisper relay / mailserver hybrid\n\n**Provides:**\n\n- likely provides scalability of up to 10k users and beyond\n- with some enhancements to partition topic logic, can possibly scale up to 1m users (app/network specific)\n\n**Caveats:**\n\n- hasn’t been tested in a large-scale simulation\n- other network and intermediate node bottlenecks might become apparent (e.g. full bloom filter and private cluster capacity; can likely be dealt with in isolation using known techniques, e.g. load balancing) (deployment specific)\n\n### Progress so far\n\nIn short, we have a [Waku version 0 spec up](https://rfc.vac.dev/spec/5) as well as a [PoC](https://github.com/status-im/nim-eth/pull/120) for backwards compatibility. In the coming weeks, we are going to solidify the specs, get a more fully featured PoC for [Waku mode](https://github.com/status-im/nim-eth/pull/114). See [rough roadmap](https://github.com/vacp2p/pm/issues/5), project board [link deprecated] and progress thread on the [Vac forum](https://forum.vac.dev/t/waku-project-and-progress/24).\n\nThe spec has been rewrittten for clarity, with ABNF grammar and less ambiguous language. The spec also incorporates several previously [ad hoc implemented features](https://rfc.vac.dev/spec/6/#additional-capabilities), such as light nodes and mailserver/client support. This has already caught a few incompatibilities between the `geth` (Go), `status/whisper` (Go) and `nim-eth` (Nim) versions, specifically around light node usage and the handshake. \n\nIf you are interested in this effort, please check out [our forum](https://forum.vac.dev/) for questions, comments and proposals. We already have some discussion for better [spam protection](https://forum.vac.dev/t/stake-priority-based-queuing/26) (see [previous post](https://vac.dev/feasibility-semaphore-rate-limiting-zksnarks) for a more complex but privacy-preserving proposal), something that is likely going to be addressed in future versions of Waku, along with many other fixes and enhancement.\n","metadata":{"layout":"post","name":"Fixing Whisper with Waku","title":"Fixing Whisper with Waku","date":"2019-12-03T12:00:00.000Z","author":"oskarth","published":true,"permalink":"/fixing-whisper-with-waku","categories":"research","summary":"A research log. Why Whisper doesn't scale and how to fix it.","image":"/img/whisper_scalability.png","discuss":"https://forum.vac.dev/t/discussion-fixing-whisper-with-waku/27"},"toc":[{"content":"Introduction","slug":"introduction","lvl":2,"i":0,"seen":0},{"content":"Whisper theoretical scalability model","slug":"whisper-theoretical-scalability-model","lvl":2,"i":1,"seen":0},{"content":"Caveats","slug":"caveats","lvl":3,"i":2,"seen":0},{"content":"Goals","slug":"goals","lvl":3,"i":3,"seen":0},{"content":"Model","slug":"model","lvl":3,"i":4,"seen":0},{"content":"Takeaways","slug":"takeaways","lvl":3,"i":5,"seen":0},{"content":"Introducing Waku","slug":"introducing-waku","lvl":2,"i":6,"seen":0},{"content":"Motivation for a new protocol","slug":"motivation-for-a-new-protocol","lvl":3,"i":7,"seen":0},{"content":"Briefly on Waku mode","slug":"briefly-on-waku-mode","lvl":3,"i":8,"seen":0},{"content":"Progress so far","slug":"progress-so-far","lvl":3,"i":9,"seen":0}]},"navProps":{"metadata":{"published":true,"title":"Fixing Whisper with Waku","layout":"post","name":"Fixing Whisper with Waku","date":"2019-12-03T12:00:00.000Z","author":"oskarth","permalink":"/fixing-whisper-with-waku","categories":"research","summary":"A research log. Why Whisper doesn't scale and how to fix it.","image":"img/whisper_scalability.png","discuss":"https://forum.vac.dev/t/discussion-fixing-whisper-with-waku/27"},"navOrder":1575374400,"localPath":"research/2019-12-03-fixing-whisper-with-waku.md","path":["fixing-whisper-with-waku"],"children":[],"isDir":false},"routeParams":{"path":["fixing-whisper-with-waku"]}},"__N_SSG":true}